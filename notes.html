The natural axes of sensory input are not intrinsically aligned with human cortical responses involved in processing natural language. The mechanisms underlying the transformation of acoustic signals to higher-order language representations remains a fundamental challenge in neuroscience. This study leverages stereotactic electroencephalography (sEEG) recordings to investigate speech processing across auditory and language features. Using canonical correlation analysis and decoding models, we demonstrate strong separability of neural populations encoding audio features and those encoding higher-order language and event structure.


Our findings reveal distinct electrode groups that accurately predict decoding performance for acoustic features and language embeddings based on topography of recording sites. Interestingly, we show that information integrated across longer timescales in artificial neural networks corresponds well with higher-order processing along the auditory pathway. These results advance our understanding of the cortical architecture enabling efficient speech comprehension and have implications for presurgical mapping and brain-computer interfaces for speech reconstruction.